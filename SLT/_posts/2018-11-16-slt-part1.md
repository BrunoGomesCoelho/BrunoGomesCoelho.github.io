---
title: "Notes Part 1"
date: 2018-11-16
tags: [machine learning, SLT]
header:
  image: "/images/icmc.jpg"
excerpt: "Notes from the beginning to part 3 of the paper 'Statistical Learning Theory: Models, Concepts, and Results'"
mathjax: "true"
--- 

This paper gives a general view of the statistical learning field, which tries to formalize knowledge, garantes and assumptions used in machine learning. Without further ado, let's get to it.





# Introduction and history
Statistical learning theory (or SLT for short) is a old field, originating in the 1960s in western Europe. The general ["father"](http://uk.businessinsider.com/facebook-just-hired-the-father-of-statistical-learning-theory-2014-11) of the field is considered to be Vladimir Vapnik, a soviet (and later american) mathematician. If you recognize his name, that is because Vapnik is also the co-inventor of Support Vector Machines (SVMs), having writen his first paper on them in the 1960 and developing the [kernel trick](https://towardsdatascience.com/understanding-the-kernel-trick-e0bc6112ef78) in the 1990s. In the coming years, he also wrote [varios](https://link.springer.com/article/10.1007/BF00994018) [papers](https://www.springer.com/us/book/9780387987804) and [books](https://www.amazon.com/Statistical-Learning-Theory-Vladimir-Vapnik/dp/0471030031) on SLT; Needless to say, his contribution to machine learning and SLT has been imense.




# The general framework of SLT
The whole idea behind "learning" tasks is to be able to infer a more general rule, that simultaneously explains past experiences and correctly classifies new examples. 

We are mentioning classification only since that is the general focus of the paper, more specifically binary classification, where we only have 2 possibles classes (which will we consider +1 and -1 from here on). We will also only consider supervised learning problems (read [this]({% post_url ML4u/2018-11-17-ml4u-video3 %}) if you don't know what means). If you are anything like me, you might be apauled to be desconsidering all of unsupervised ande semi-supervised learning, but according to the paper it will make the maths easier (and we will be desconsidering so many other things, this will be the least of our worries).

More spefically, SLT theory tries to answer the following questions about learning tasks:

* Which learning tasks can machines perform? 
* To be able to garante that we can learn a task, what assumptions must we make about the task?
* For a machine learning algorithm to be able to learn a task, what proprierties does it need to have?
* Can we give any perfomance garantes to our ML algorithms?


# The formal setup

To setup the framework we will use to help us answer some of the above questions, we will need to formalize some things:
Lets consider our data is divided into two parts, where X represents the information we have as to help us predict something (our feature space) and Y our prediction (target space).
Lets assume there is a underlying [joint probability distribution](http://www.inf.ed.ac.uk/teaching/courses/cfcs1/lectures/joint.pdf) P on X * Y (that is, P is the distribution of the probability of X=x with intersection with Y=y). Also assume X and Y are [independent and identically distributed random variables (iid)](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables). That means that each instance of our data, (Xi, Yi), is sampled independently from P, with one sampling not affecting the next.

We will be able to do many things with just these assumptions, but first lets have a look at what it truly means to suppose the things we did:

1. **We did not assume P to be any known distribution.** This is important because frequently in statistics we suppose certain distributions, but our whole work here won't actually try to define the distribution P

2. **Our labels can be non-deterministic.** Since P can be any distribution, we are including the case where we don't have a way to predict with 100% accuracy some instaces of our data. This can be seen from the fact that $$ P(Y=+1 | X=x) $$ does not need to be 0 or 1, instead being able to be any value in between as well. This is a needed property to have since we might have *noise* in our data, not allowing us to be 100% sure, or maybe we have *overlapping classes*.

For example consider we are trying to predict the sex of a individual given his height; depending on our data, it will be impossible to be 100% sure of the person's sex if their height is 1.7m (in fact, given a large enough sample, no height would give us 100% accuracy).

If there isn't much noise in the data, 




