---
title: "Notes SLT Part 1"
date: 2018-11-16
tags: [machine learning, SLT]
header:
  image: "/images/icmc.jpg"
excerpt: "Notes from the beginning to chapter 3 of the paper 'Statistical Learning Theory: Models, Concepts, and Results'"
mathjax: "true"
--- 

[This paper](https://arxiv.org/pdf/0810.4752.pdf) gives a general view of the statistical learning field, which tries to formalize knowledge, garantes and assumptions used in machine learning. While I tried to split my notes on it in various parts, they will still be pretty long, make sure to take your time with them and only move on after understanding each concept.




# Introduction and history
Statistical learning theory (or SLT for short) is a old field, originating in the 1960s in western Europe. The general ["father"](http://uk.businessinsider.com/facebook-just-hired-the-father-of-statistical-learning-theory-2014-11) of the field is considered to be Vladimir Vapnik, a soviet (and later American) mathematician. If you recognize his name, that is because Vapnik is also the co-inventor of Support Vector Machines (SVMs), having written his first paper on them in the 1960 and developing the [kernel trick](https://towardsdatascience.com/understanding-the-kernel-trick-e0bc6112ef78) in the 1990s. In the coming years, he also wrote [various](https://link.springer.com/article/10.1007/BF00994018) [papers](https://www.springer.com/us/book/9780387987804) and [books](https://www.amazon.com/Statistical-Learning-Theory-Vladimir-Vapnik/dp/0471030031) on SLT; Needless to say, his contribution to machine learning and SLT has been immense.




# The general framework of SLT
The whole idea behind "learning" tasks is to be able to infer a more general rule, that simultaneously explains past experiences and correctly classifies new examples. 

We are mentioning classification only since that is the general focus of the paper, more specifically binary classification, where we only have 2 possibles classes (which will we consider +1 and -1 from here on). We will also only consider supervised learning problems (read [this]({% link _ml/2018-11-17-ml4u-video3.md %}) if you don't know what means). If you are anything like me, you might be appalled to be disconsidering all of unsupervised and semi-supervised learning, but according to the paper it will make the maths easier (and we will be disconsidering so many other things, this will be the least of our worries).

More specifically, SLT theory tries to answer the following questions about learning tasks:

* Which learning tasks can machines perform? 
* To be able to guarantee that we can learn a task, what assumptions must we make about the task?
* For a machine learning algorithm to be able to learn a task, what properties does it need to have?
* Can we give any performance guarantees to our ML algorithms?


# The formal setup

### Considerations about our data
To setup the framework we will use to help us answer some of the above questions, we will need to formalize some things:
Lets consider our data is divided into two parts, where X represents the information we have as to help us predict something (our feature space) and Y our prediction (target space).

Lets assume there is a underlying [joint probability distribution](http://www.inf.ed.ac.uk/teaching/courses/cfcs1/lectures/joint.pdf) $$ P $$ on $$ X \times Y $$ (that is, P is the distribution of the probability of X=x with intersection with Y=y or in mathematical jargon, $$ P(X=x \| Y=y) $$). Also assume X and Y are [independent and identically distributed random variables (iid)](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables). That means that each instance of our data, $$ (X_i, Y_i) $$, is sampled independently from P, with one sampling not affecting the next.

We will be able to do many things with just these assumptions, but first lets have a look at what it truly means to suppose the things we did:

1. **We did not assume P to be any known distribution.** This is important because frequently in statistics we suppose certain distributions, but our whole work here won't actually try to define the distribution P

2. **Our labels can be non-deterministic.** Since P can be any distribution, we are including the case where we don't have a way to predict with 100% accuracy some instances of our data. This can be seen from the fact that $$ P(X=x \| Y=y) $$ does not need to be 0 or 1, instead being able to be any value in between as well. This is a needed property to have since we might have *noise* in our data, not allowing us to be 100% sure, or maybe we have *overlapping classes*.
For example consider we are trying to predict the sex of a individual given his height; depending on our data, it will be impossible to be 100% sure of the person's sex if their height is 1.7m (in fact, given a large enough sample, no height would give us 100% accuracy).
If there isn't much noise in the data, then $$ n(x) = P(X=x \| Y=y) $$ will be either close to 0 or +1 (depending on if the true label of x is -1 or +1). But if there is a lot of noise in the data, $$ n(x) \approx 0.5 $$, which means that trying to predict the class is the same as a random guess.

3. **Independent sampling.** It is a reasonable assumption to make, but frequently not the case in real data. For example, take the drug pharmacy, where ML is used to try to predict if a certain combination of chemicals will be useful for humans. Since it is very costly to actually test all of these combinations, the data we do have is based on what experts thought was worth testing in the first case, clearly not a independent sampling of all the possible chemical options.
We can also notice that this property means we can not use this framework (at least not without a more general formulation) to analyze [active learning](https://en.wikipedia.org/wiki/Active_learning_(machine_learning)).

4. **The distribution of P is fixed.** Because of this, we are excluding pretty much all time-series problems and problems related to co-variate shift, that is, when the distributions of independent variables change due to some variable we are not measuring (for example, the height of a population changing due to better access to food, but there still being a relationship between height and sex).

5. **The distribution of P is unknown.** This is important because we will learn that if P was known, we would have a closed formula for the best classifier. In ML, precisely what we want to solve are problems where P is not known.


Okay, that was a lot of thought provoking concepts, take some time to sketch them out and make sure you know what the implications of our assumptions really mean. With that done, let's now get into some more concrete definitions, which will be needed to help us evaluate how well a classifier performs:


# Definitions 
We will treat our classifier as a function/mapping of $$ X -> Y $$ and call it  $$ f $$.

We need a way to measure how a classifier performs, more specifically what is the error it makes on a single individual data point. The general name for this functions is a **loss function**. For example, in binary classification, our error can simply be 0 if we have predicted the right class and +1 otherwise. For regression problems, a frequently used loss function is the [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error). Given a certain classifier $$ f $$, our loss function is then: $$ L(X, Y, f(X)) $$.

So now that we have a function to measure the error of a instance in our data, what is our overall error? It can be defined as simply the [expected value](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/expected-value/) of the loss function, frequently called the **risk of a function** and defined as $$ R(f) = E(L(X, Y, f(x)))$$, where $$ E() $$ represents the expected value of something.

With these definitions, we realize we want a classifier $$ f $$ that has the smallest possible $$ R(f) $$.


## A optimal classifier

Just remembering, we want a function $$ \{f: X - > Y\} $$ given a joint distribution P between X and Y. We can easily create a optimal classifier, named **Bayes Classifier** if we definite it as:

$$ f_{BAYES} = +1 $$     if $$ P(X=x \| Y=y) >= 0.5 $$

$$ f_{BAYES} = -1 $$       otherwise

That's great, if we can create such a classifier, all our problems are solved right? Actually, we have just created another problem: remember that the underlying distribution P is not known, therefore we can't actually create our classifier. Luckily for us, SLT comes to the rescue again! Let's first take a breather, get some coffee and properly define out classification problem with all the definitions we have now created:


## Binary classification definition

*Given some training points $$ (X_i, Y_i), ..., (X_n, Y_N) $$ drawn independently from some *unknown*, non-changing distribution P, and given a *loss function* $$ l() $$, how can we construct a function $$ f: X - > Y $$ which has a risk $$ R(f) $$ as close as possible to the risk of the Bayes Classifier?*

That's good, we have formalized exactly what the binary classification problem is! Also, we know exactly what the problem is:

* We cannot compute $$ R(f_{BAYES}) $$ without knowing exactly the distribution P.

How does STL theory deal with this? As you've probably figured it out by now, by creating even more definitions and mathematical formalizations! Let's try to figure out what it means for a classifier to be able to "generalize" as well as the Bayes Classifier.


# Generalization

Remember that we want a classifier *f* that is able to learn from data and generalize to some unseen data, but what does "generalize" even mean in this context? Intuitively it means that it should have the same expected value for the error in both our training data and "all" the available data. We can write this out mathematically if we define the **empirical risk/training error** as:

$$ R_{emp}(f) = 1/n \sum_{i=1}^n l(X_i, Y_i, f(X_i)) $$

With this, a good generalization can then be written as:

$$  \left\lvert R(fn) - R_{emp}(f) \right\rvert â‰ˆ 0 $$

Notice that this does not mean that our classifier *fn* is a good classifier and makes few errors (low $$ R_{emp} $$ ), just that it generalizes well to unseen data meaning that whatever its accuracy is on our test set, it will have roughly the same accuracy for new data. 

From this definition comes one of the most important insights in statistics/machine learning: **the bias-variance dilema**.

Consider our data has only two features and that we create two different classifiers, a simple one represented by a straight line and a more complex one represented by some polynomial function:

<figure>
	<img src="https://upload.wikimedia.org/wikipedia/commons/6/68/Overfitted_Data.png">
	<figcaption><a href="https://en.wikipedia.org/wiki/Overfitting#/media/File:Overfitted_Data.png" title="Courtesy of Ghiles">Courtesy of Ghiles, on creative commons</a>.</figcaption>
</figure>

Notice that for a function to have a low error rate on our training set, it must adapt itself to all the nuance in our data which will make it a more complex function. But this might mean that it picks up on random noise, which will make it not generalize well (also called overfitting, from the idea that the function approximates too much the points we have given it). This is the *variance* aspect, meaning that the variance in new unseen data will confuse our function.


If we instead try to only use simpler functions, we will be able to generalize well and have a similar error for both our test set and unseen data. The problem here is the fact that we don't know if the space of simples functions contains the best possible function, we are introducing a *bias* in the search space of functions, lending ourselves to underfitting the data. 


Therefore we would like to answer the question *"What is the amount of increase in training error we should tolerate for a simpler model?"*. Fortunately for us (since otherwise a data science career might not exist) this is a open problem, without a clear answer and [many](https://www.datarobot.com/wiki/training-validation-holdout/), [many](https://en.wikipedia.org/wiki/Bootstrapping), [many](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) ways to go about trying to deal with it.

So to summarize what we have learned:

* A complex function leads to overfitting to our training data, giving us a worse risk on new data due to the variance in it.
* A simple function leads to underfitting our function, also giving us a worse risk due to the bias we are adding with the assumptions we make about the classifier being simple.

Let's now have a look at another import aspect, the idea of **consistency**, that will serve to further enlighten us about the bias-variance dilemma.

# Consistency

Intuitively, consistency means that the more data we give a classifier during its training, the better it should be able to generalize to new data. In fact, when we give it infinite data, it should theoretically have the same error as the best possible classifier we could have come up with.

This best classifier might be the Bayes Classifier if it is included in our function search space, but remember that in machine learning we will frequently not search over all possibles functions, instead introducing some bias as to what functions we will consider.

To formalize, given a space $$ \mathcal{F} $$ of functions, the best function in $$ \mathcal{F} $$ can be written as:

$$ f_{\mathcal{F}} = \underset{f \in \mathcal{F}}{argmin R(f)} $$

Alongside this definition, we will use $$ fn $$ to mean a trained function in $$ \mathcal{F} $$, using "n" datapoints;

and $$ f_{BAYES} $$ is our old friend, the best possible classifier if we considerer the whole of the search space of functions, $$ \mathcal{F}_{all} $$.

With all of this, we can now define consistency, which can be done in 3 different ways depending on what statistics book you pickup... Let's check out all of them:

1. A classifier is *consistent with regards to $$ \mathcal{F} $$ and P* (our joint distribution between X and Y) if it approaches the best possible classifier in $$ f_{\mathcal{F}} $$, that is, $$ \forall \epsilon > 0, Prob (R({fn}) - R({f\mathcal{F}}) > \epsilon) > 0 $$ as n approaches $$ + \inf $$


2. Analogously, a classifier is *Bayes consistent with regards P* if it approaches the best possible classifier in $$ \mathcal{F}_{all} $$, that is, $$ \forall \epsilon > 0, Prob (R({fn}) - R({f_{BAYES}}) > \epsilon) > 0 $$ as n approaches $$ + \inf $$


3. Finally, a classifier is *universally consistent*  (either with regards with $$ \mathcal{F} $$ or $$ \mathcal{F}_{all} $$) if it approaches the best possible classifier in its given space for *all possible distributions P*. 

For us, definition 3 is the most interesting one since it doesn't depend on a specific P, a interesting property since one of our core ideas is that we are creating algorithms to figure out P, there being no point to all of this if we already knew P.

Notice that to calculate our risk, we are using the true risk value, $$ R(fn) $$ but ideally the same should happen to $$ R_{emp}(fn) $$ since that is all we will have access to during the training of the algorithm. Luckily for us we don't really need to worry about this, since it will end up being a byproduct of trying to prove consistent, that is, we will show that if a classifier is consistent with the true risk value, it is also consistent using our empirical risk.


# The bias-variance dilemma and the estimation-approximation trade-off
After defining consistency, we could now try to define our classifier as:

$$ fn = \underset{f \in \mathcal{F}_{all}}{argmin R_{emp}(f)} $$

Here lies a problem though: it can be shown that if the space $$ f_{\mathcal{F}} $$ contains all the Bayesian Classifiers for all the different joing probability distributions P, then it is impossible for our classifier to be consistent.

The idea behind this is realizing that $$ R({fn}) - R({f_{BAYES}}) $$ can be re-written as:

$$ \{ R({fn}) - R({f\mathcal{F}}) \} + \{ R({fn}) - R({f_{BAYES}})) \} $$

* The first term of the equation is called our *estimation error* since it comes from the fact that we are only given a finite number of "n" points to estimate the best function in $$ \mathcal{F} $$. It is associated with the uncertainty (the variance, our old friend from the variance-bias dilemmma) of our data, introduced by the random sampling which we have access to.

* The second term is the **approximation error**, associated with the bias of considering a $$ \mathcal{F} $$ that is smaller than $$ \mathcal{F}_{all} $$.

Therefore, choosing the appropriate space $$ \mathcal{F} $$ for a algorithm is exactly how we balance the trade-off between estimation error and approximation error, since in practice we often can't just acquire better (or more) data.

If we choose a large $$ \mathcal{F} $$, our approximation error will be small, but since we are including complex functions, we will have a tendency to overfit on our "n" samples, giving us a higher estimation error.

If $$ \mathcal{F} $$ is small, the opposite will happen, we will learn a simpler function, but it won't be as close to the optimal.

# Congratulations!

If you've gotten to this point, congratulations on sticking through all the maths and definitions. All of this so far has been very theoretical, but we have been able to gain some amazing insights from it all. If you didn't understand everything all at once that's perfectly normal, give it a few days and come back to reread the post, also try to explain everything in our own words (or even better, try to teach a friend!).

From here on, we will have a look at a more practical example, proving that a [well known and used](https://scikit-learn.org/stable/modules/neighbors.html) algorithm is in fact consistent.

I would suggest first having a look at the [explanation behind the K Nearest Neighbours (K-NN)]({% link _ml/2018-11-17-ml4u-video5_6_7.md %}) algorithm, which accompanies a implementation in Python3.

If you already have the idea behing K-NN at the tip of your tongue, go ahead and read on [about the proof that it is a consistent algorithm]({% link _ml/2018-11-16-slt-part2.md %}).
